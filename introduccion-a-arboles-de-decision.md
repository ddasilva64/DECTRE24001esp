# Introducci√≥n a √°rboles de decisi√≥n

<details>

<summary>√çndice de contenidos</summary>

* ***¬øQu√© son los √°rboles de decisi√≥n?***   
  * ***Terminolog√≠a*** 
  * ***Reglas***  
  * ***Algoritmos***  
    * ***Conceptos***  
    * ***Utilizaci√≥n***  
* ***Pasos para implementar √°rboles de decisi√≥n***  
  * ***1) Recopilaci√≥n y preparaci√≥n de datos***  
  * ***2) Selecci√≥n de caracter√≠sticas***  
  * ***3) Entrenamiento del modelo***  
  * ***4) Validaci√≥n del modelo***  
  * ***5) Ajuste y optimizaci√≥n del modelo***  
  * ***6) Implementaci√≥n y despliegue***  
  * ***7) Adicionalmente***  
    * ***7.1) Seleccionar el algoritmo adecuado***  
    * ***7.2) Considerar la interpretabilidad del modelo***  
    * ***7.3) Realizar una validaci√≥n exhaustiva***
    * ***7.4) Evaluar la estabilidad del modelo***  
  * ***Matriz de confusi√≥n***
    * ***M√©tricas***
      * ***Accuracy (exactitud)***  
      * ***Precision (precisi√≥n)***  
      * ***Bias, √≥ inaccuracy (sesgo)***    
      * ***Recall o sensitivity (sensibilidad)***  
      * ***Especificity (especificidad)***  
      * ***Relaci√≥n entre recall (sensibilidad) y especificity (especificidad)***
      * ***Mediciones de la precisi√≥n de una prueba***
        * ***Tasa de falsos positivos (TFP)***  
        * ***Tasa de falsos negativos (TFN o tasa de error)***  
        * ***Tasa de verdadero positivos (TVP o sensibilidad)***  
        * ***Tasa de verdaderos negativos (TVN o especificidad)***  
        * ***F1 Score***  
      * ***Resumen***
    * ***Consejos generales sobre la matriz de confusi√≥n y sus m√©tricas***  
    * ***Balanceo de clases***  
    * ***Hiperpar√°metros***  
* ***¬øC√≥mo funcionan los √°rboles de decisi√≥n?***  
  * ***Ventajas***  
  * ***Desventajas***  
  * ***¬øCu√°ndo usar √°rboles de decisi√≥n?***  
  

</details>

## ¬øQu√© son los √°rboles de decisi√≥n?  

***[<span style="font-family:Verdana; font-size:0.95em;color:red">√Årbol de decisi√≥n</span>][r000]***: ***Modelo de [<span style="font-family:Verdana; font-size:0.95em;color:red">ML</span>][r001] [<span style="font-family:Verdana; font-size:0.95em;color:red">supervisado</span>][r002]*** para ***[<span style="font-family:Verdana; font-size:0.95em;color:red">clasificaci√≥n</span>][r005] y [<span style="font-family:Verdana; font-size:0.95em;color:red">regresi√≥n</span>][r005]***. *Muy extendidos por su  simplicidad, facilidad de interpretaci√≥n y aplicaciones diversas*.  

![Aprendizaje supervisado][i001]  

![Regresi√≥n vs clasificaci√≥n 001][i002]  

![Regresi√≥n vs clasificaci√≥n 002][i003]  

![Regresi√≥n vs clasificaci√≥n 003][i004]  

Las primeras versiones de los *[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]* fueron propuestas por [<span style="font-family:Verdana; font-size:0.95em;color:red">*Leo Breiman*</span>][r003] en la d√©cada de 1980.  

![Leo Breiman][i000]    

> Fuente: [<span style="font-family:Verdana; font-size:0.95em;color:red">*Decision Trees y Random Forest*</span>][r004]   

### Terminolog√≠a

* **Nodo**: El momento en el que se ha de tomar una decisi√≥n de entre varias posibles, lo que va haciendo que a medida que aumenta el n√∫mero de **nodos** aumente el n√∫mero de posibles finales a los que puede llegar. Esto hace que un ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** con muchos **nodos** sea complicado de dibujar a mano y de analizar debido a la existencia de numerosos caminos que se pueden seguir. Dicho de otro modo, *un **nodo** es una pregunta sobre los datos*.  

![Nodo][i005]  

* **Nodo ra√≠z**: Es el primer **nodo** del ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** y no tiene un **nodo padre**. Representa el conjunto de datos completo y es el punto de partida para la construcci√≥n del **√°rbol**.  

![Nodo ra√≠z][i006]    

* **Rama**: Es una conexi√≥n entre dos **nodos** del **√°rbol** que representa una opci√≥n posible para un atributo determinado.  

![Rama][i007]  

* **Nodo de decisi√≥n**: Es un **nodo** interno del **√°rbol** que representa una caracter√≠stica del conjunto de datos y tiene al menos dos **ramas** que indican las posibles opciones para esa caracter√≠stica.  

![Nodo de decisi√≥n][i008]   

* **Divisi√≥n**: Es la separaci√≥n de un conjunto de datos en subconjuntos m√°s peque√±os y homog√©neos en funci√≥n de un atributo determinado. Cada divisi√≥n genera un nuevo **nodo** y una nueva **rama** en el **√°rbol**.  

![Divisi√≥n][i009]    

* **Nodo padre**: Es el **nodo** inmediatamente anterior a un **nodo** espec√≠fico en el **√°rbol**. Un **nodo** puede tener varios **nodos hijos**, pero solo tiene un **nodo padre**.  

![Nodo padre][i010]    

* **Nodo hijo**: Es un **nodo** inmediatamente posterior a un **nodo** espec√≠fico en el **√°rbol**. Un **nodo** puede tener varios **nodos hijos**, pero solo tiene un **nodo padre**.  

![Nodo hijo][i011]    

* **Nodo hoja**: Es un **nodo** terminal del **√°rbol** que no tiene m√°s **ramas** y representa una **etiqueta de clase** o una predicci√≥n para el conjunto de datos de entrada.  

![Nodo hoja][i012]    

* **Overfitting (sobreajuste)**: Se produce cuando *los datos de formaci√≥n son pocos o contienen incoherencias*.  

![Overfitting][i013] 

* **Pruning (poda)**: Es un proceso de eliminaci√≥n de **nodos** innecesarios del **√°rbol** para evitar el **overfitting** (**sobreajuste**) del modelo y mejorar su capacidad de generalizaci√≥n. La **poda** se realiza despu√©s de construir el **√°rbol** y puede ser *prepruning o postpruning*. Es decir, se *elimina una **rama** de un **nodo** transform√°ndolo en una **hoja***. Una met√°fora ser√≠a intentar avanzar a un cami√≥n sin poderlo ver (podemos tener problemas).  

![Pruning][i014] 

* **Vectores** (de n√∫meros): La soluci√≥n final a la que se llega en funci√≥n de las diversas posibilidades que se tienen, dan las utilidades en esa soluci√≥n.  

![Vectores][i015]  

* **Flechas**: Uniones entre un **nodo** y otro **nodo** y representan cada acci√≥n distinta.  

![Flechas][i016]   

* **Etiquetas**: Se encuentran en cada **nodo** y cada **flecha** y dan nombre a cada acci√≥n.

![Etiquetas][i017]   

* **Clase**: Un ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** es una representaci√≥n simple para **clasificar** ejemplos. Para esta secci√≥n, supongamos que todas las caracter√≠sticas de entrada tienen dominios discretos finitos y que hay una √∫nica caracter√≠stica objetivo llamada "**clasificaci√≥n**". Cada elemento del dominio de la **clasificaci√≥n** se denomina **clase**.  

![Clase][i018]  

* **Costo**: 
    * **Costo de medici√≥n**: Para determinar el *valor de una determinada propiedad (atributo)* exhibida por el objeto.
    * **Costo de clasificaci√≥n err√≥nea**: Al *decidir que el objeto pertenece a la clase X cuando su clase real es Y*.  

* **Validaci√≥n cruzada**: Es el proceso de *construir un ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** con la mayor√≠a de los datos y posteriormente usar la parte restante de los datos para probar la precisi√≥n del **[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]***.  

![Validaci√≥n cruzada][i019] 

* **Outliers**: Valores at√≠picos de la muestra.  

![Outliers][i020]   

El modelo aprende de los datos, generando reglas de tipo *if-else*. Cada **rama** del **√°rbol** es una respuesta a un **nodo**. El proceso contin√∫a hasta que se llega a una **hoja** del **√°rbol**, la cual representa la predicci√≥n final (**vectores**).  

* ***Tipos de variables en Estad√≠stica***

* **Variables cualitativas**: Son caracter√≠sticas de un individuo u objeto, que ***se pueden expresar con palabras***. Algunos ejemplos son: el color de ojos, el color del cabello, el g√©nero, el estado civil o la marca de un producto. 

* **Variables cuantitativas**: Son aquellas caracter√≠sticas de un objeto o individuo que ***se pueden escribir en n√∫meros***. Por ejemplo: edad, ingresos, peso, altura, presi√≥n, humedad o cantidad de hermanos.  

  * **Variables discretas**: Son aquellas que ***no aceptan un valor entre dos n√∫meros consecutivos***. Si tienes los datos 1, 2, 3, 10, 11 y 15, entre el 1 y 2 no puede aparecer el 1.48, porque del 1 salta directamente al 2. Generalmente, las variables discretas son resultado de un conteo y no permiten los n√∫meros decimales. Por ejemplo: n√∫mero de pacientes, n√∫mero de alumnos, n√∫mero de motos por modelo.  

  * **Variables continuas**: Son aquellas que ***pueden tomar cualquier valor entre dos intervalos o n√∫meros***. Por ejemplo, si necesitas escribir la estatura de un grupo de basquetbolistas, seguramente, no podr√°s utilizar los n√∫meros 1 y 2, pero si las variables 1.78, 1.65, 1.45, porque la altura suele expresarse de esa manera.
  
### Reglas

1) *Al comienzo del juego hay un **nodo** inicial que no es apuntado por ninguna **flecha***, es el √∫nico **nodo** del juego con esta caracter√≠stica.  

2) El *resto de los **nodos** del juego son apuntados por una √∫nica **flecha***.  

3) De lo anterior se deduce que *hay un √∫nico camino para llegar del **nodo** inicial a cada uno de los **nodos** del juego*. Las decisiones son excluyentes y no hay marcha atr√°s. Opci√≥n 1->opci√≥n 2->opci√≥n 3->Resultado Final X  

### Algoritmos  

  * ***[<span style="font-family:Verdana; font-size:0.95em;color:red">ID3</span>][r008] (Iterative Dichotomiser 3)***: A [<span style="font-family:Verdana; font-size:0.95em;color:red">Ross Quinlan</span>][r009] se le atribuye el desarrollo de ***[<span style="font-family:Verdana; font-size:0.95em;color:red">ID3</span>][r008]***. Este algoritmo aprovecha la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** y la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Ganancia de la informaci√≥n</span>][r012]*** como **m√©tricas** para evaluar las divisiones de candidatos.   

    * **Algoritmo**:  
  
      1) Calcular la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** para todas las **clases**.  

      2) Seleccionar el mejor **atributo** basado en la reducci√≥n de la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]***.  

      3) *Iterar hasta que todos los objetos sean clasificados*.  

  * ***[<span style="font-family:Verdana; font-size:0.95em;color:red">C4.5</span>][r010]***: Este algoritmo se considera una iteraci√≥n posterior de ***[<span style="font-family:Verdana; font-size:0.95em;color:red">ID3</span>][r008]***, que tambi√©n fue desarrollado por [<span style="font-family:Verdana; font-size:0.95em;color:red">Ross Quinlan</span>][r009]. Puede utilizar la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Ganancia de la informaci√≥n</span>][r012]*** o las proporciones de la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Ganancia de la informaci√≥n</span>][r012]*** para evaluar los puntos de divisi√≥n dentro de los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]***.  

    * **Algoritmo**:  

      1) En cada **nodo** del ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]***, ***[<span style="font-family:Verdana; font-size:0.95em;color:red">C4.5</span>][r010]*** elige un **atributo** de los datos que m√°s eficazmente dividen el conjunto de muestras en subconjuntos enriquecidos en una **clase** u otra.  

      2) Su criterio es el normalizado para la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Ganancia de la informaci√≥n</span>][r012]*** (diferencia de ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]***) que resulta en la elecci√≥n de un atributo para dividir los datos.  

      3) El atributo con la mayor ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Ganancia de la informaci√≥n</span>][r012]*** normalizada se elige como par√°metro de decisi√≥n.  

      4) El algoritmo ***[<span style="font-family:Verdana; font-size:0.95em;color:red">C4.5</span>][r010]*** divide recursivamente en sublistas m√°s peque√±as.  

  * ***[<span style="font-family:Verdana; font-size:0.95em;color:red">CART (*Classification And Regression* Trees)</span>][r015]***: Fue introducido por [<span style="font-family:Verdana; font-size:0.95em;color:red">*Leo Breiman*</span>][r003]. Generalmente utiliza la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">impureza de Gini</span>][r018]***, la cual ***mide la frecuencia con la que se clasifica incorrectamente un atributo elegido al azar***, con lo que *se identifica el atributo ideal para la divisi√≥n*, es decir, *un valor m√°s bajo es m√°s ideal*.   

    * **Algoritmo**:  

      1. Para crear la **ra√≠z del ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]*****, es decir la primera partici√≥n, se toman todas las caracter√≠sticas y, para cada una de ellas, se definen todos los posibles umbrales a que haya lugar. Cada umbral ser√° simplemente el punto intermedio entre dos valores consecutivos de cada caracter√≠stica.  

      2. Para cada uno de estos umbrales se calcula la partici√≥n (**nodo** izquierdo y **nodo** derecho) y para cada **nodo hijo** se calcula el ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√≠ndice de Gini</span>][r018]***. Con estos **nodos hijos** se calcula la funci√≥n de **costo del nodo padre**, que es el promedio ponderado de los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√≠ndices de Gini</span>][r018]*** de sus hijos.  

      3. Se toma el umbral (o **nodo padre** resultante) que tenga la funci√≥n de **costo** con el menor valor posible, indicando que la partici√≥n obtenida es la m√°s homog√©nea de todas las analizadas.  

      4. Una vez se haya realizado esta partici√≥n, se repite el mismo procedimiento de forma iterativa para los **nodos** resultantes, exceptuando los que sean **nodos hoja**.

#### Conceptos

  * ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Entrop√≠a de la informaci√≥n</span>][r007]***: La cantidad de informaci√≥n promedio que contienen los s√≠mbolos usados. ***Los s√≠mbolos con menor probabilidad son los que aportan mayor informaci√≥n***; por ejemplo, si se considera como sistema de s√≠mbolos a las palabras en un texto, palabras frecuentes como ¬´que¬ª, ¬´el¬ª, ¬´a¬ª aportan poca informaci√≥n, mientras que palabras menos frecuentes como ¬´corren¬ª, ¬´ni√±o¬ª, ¬´perro¬ª aportan m√°s informaci√≥n. Si de un texto dado borramos un ¬´que¬ª, seguramente no afectar√° a la comprensi√≥n y se sobreentender√°, no siendo as√≠ si borramos la palabra ¬´ni√±o¬ª del mismo texto original. Cuando todos los s√≠mbolos son igualmente probables (distribuci√≥n de probabilidad plana), todos aportan informaci√≥n relevante y la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** es m√°xima. La ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** puede ser considerada como una medida de la incertidumbre y de la informaci√≥n necesaria para, en cualquier proceso, poder acotar, reducir o eliminar la incertidumbre. Es decir, ***cuanto mayor sea la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]***, mayor ser√° la impureza y la mezcla de clases en el conjunto de datos***.  

  ![Entrop√≠a de la informaci√≥n][i021]  

  * *Definici√≥n formal*  

    Supongamos que un evento (variable aleatoria) tiene un *grado de indeterminaci√≥n inicial igual a ùëò* (i.e. existen ùëò estados posibles) y supongamos *todos los estados equiprobables*. Entonces *la probabilidad de que se d√© una de esas combinaciones ser√° $p=\frac{1}{k}$*. Luego podemos representar la expresi√≥n $ùëê_ùëñ$ como:  

      $$
      \begin{align*}  
      c_i &= log_2(k)\\  
          &= log_2[\frac{1}{(\frac{1}{k})}]\\  
          &= log_2(\frac{1}{p})\\ 
          &= log_2(1)-log_2(p)\\  
          &= 0-log_2(p)\\  
          &= -log_2(p) 
      \end{align*}
      $$

    Si ahora *cada uno de los ùëò estados tiene una probabilidad $ùëù_ùëñ$, entonces la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** vendr√° dada por la suma ponderada de la cantidad de informaci√≥n*:  

      $$
      \begin{align*}
      H &= -p_1log_2(p_1)-p_2log_2(p_2)-...-p_klog_2(p_k)\\  
        &= -\sum_{i=1}^{k}p_ilog_2(p_i)
      \end{align*}
      $$  

    Por lo tanto, la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** de un mensaje ùëã, denotado por ùêª(ùëã), **es el valor medio ponderado de la cantidad de informaci√≥n de los diversos estados del mensaje**:  

      $$
      \begin{align*}
      H(X) &= -\sum_{i}p(x_i)log_2p(x_i)\\  
           &= \sum_{i}p(x_i)log_2p(\frac{1}{p(x_i)})  
      \end{align*}
      $$  

    que **representa una medida de la incertidumbre (impureza) media acerca de una variable aleatoria y por tanto de la cantidad de informaci√≥n**.

    Es decir,  

      Entrop√≠a(S) $=-p(+)log_2(p(+))-p(-)log_2(p(-))$  

    donde  

    * $p_i$ es la proporci√≥n de ejemplos en la clase i en el conjunto de datos S.  
    * p(+) son √©xitos.
    * p(-) son fracasos.

  * ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√çndice (o impureza) de Gini</span>][r018]***: Mide la frecuencia con la que se clasifica incorrectamente un atributo elegido al azar. Cuando se eval√∫a usando ***la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">impureza de Gini</span>][r018]***, un valor m√°s bajo es ideal***. Es decir, ***al igual que con la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]***, cuanto menor sea la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">impureza de Gini</span>][r018]***, mayor ser√° la pureza del conjunto de datos***.  
  
  * ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Ganancia de la informaci√≥n</span>][r012]***: Representa la diferencia de ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** antes y despu√©s de una divisi√≥n en un atributo determinado. *El atributo con la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Ganancia de la informaci√≥n</span>][r012]*** m√°s alta producir√° la mejor divisi√≥n*, ya que hace el mejor trabajo al clasificar los datos de entrenamiento de acuerdo con su clasificaci√≥n de destino. Es decir, ***nos interesan valores m√°s altos de la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Ganancia</span>][r012]******.

    **Ejemplos**:  

    * **La ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** de un mensaje M de longitud 1 car√°cter que utiliza el conjunto de caracteres ASCII, suponiendo una equiprobabilidad en los 256 caracteres ASCII, ser√°:**  

      $$
      \begin{align*}
      H(M) &=-\sum_{i}^{256}\frac{1}{256}log_2\frac{1}{256}\\  
           &=(-\frac{256}{256})log_2(\frac{1}{256})\\  
           &=-1(log_2(1)-log_2(256))\\  
           &=0+log_2(256)\\   
           &=log_2(256)\\  
           &=8\\  
      \end{align*}
      $$  

    * **Supongamos que el n√∫mero de estados de un mensaje es igual a 3, $M_1$, $M_2$ y $M_3$ donde la probabilidad de $M_1$ es 50%, la de $M_2$ 25% y la de $M_3$ 25%. Por tanto, la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** es:**  

      $$
      \begin{align*}
      H(M)&=\frac{1}{2}log_2(2)+\frac{1}{4}log_2(4)+\frac{1}{4}log_2(4)\\  
          &=\frac{1}{2}¬∑1+\frac{1}{4}¬∑2+\frac{1}{4}¬∑2\\  
          &=\frac{1}{2}+\frac{1}{2}+\frac{1}{2}\\  
          &=\frac{3}{2}\\  
          &=1,5 
      \end{align*}
      $$ 

    * **Tenemos un conjunto de estudiantes, en el que vamos a considerar las siguientes caracter√≠sticas:** 

    ![Ejemplo estudiantes][i022] 

    * El conjunto total es de 30 estudiantes  
    * Consideramos 3 variables: G√©nero (hombre/mujer), Clase (IX/X) y Altura (5,5pies=1,68m).  
    * Del conjunto total 15 estudiantes juegan al cricket en su tiempo libre
    * *Debemos crear un modelo para predecir quien jugar√° al cricket*  
    * Segregar estudiantes basados en todos los valores de las 3 variables e identificar aquella variable que crea los conjuntos m√°s homog√©neos de estudiantes y que a su vez son heterog√©neos entre ellos.  

    Obtenemos los siguientes resultados experimentales:  

    * 10 estudiantes son mujeres y 20 son hombres    
    * 14 estudiantes son de la clase IX y 16 son de la clase X  
    * 12 estudiantes miden < 1,68m y 18 miden $\ge$ 1,68m

    Consideramos dos categor√≠as en cada clase, √©xito y fracaso, con la f√≥rmula:  

    Entrop√≠a(S) $=-p(+)log_2(p(+))-p(-)log_2(p(-))=>$ √©xitos y fracasos  

    * Un 20% de las mujeres juegan al cricket, el complemento (jueguen o no) es del 80%.  
    * Un 65% de los hombres juegan al cricket, el complemento (jueguen o no) es del 35%.   
    * Un 43% de los/as alumnos/as de la clase IX juegan al cricket, el complemento (jueguen o no) es del 57%.  
    * Un 56% de los/as alumnos/as de la clase X juegan al cricket, el complemento (jueguen o no) es del 44%.   
    * Un 42% de los/as alumnos/as que miden < 1,68m juegan al cricket, el complemento (jueguen o no) es del 58%.  
    * Un 56% de los/as alumnos/as que miden $\ge$ 1,68m juegan al cricket, el complemento (jueguen o no) es del 44%.   

  C√°culo del ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√≠ndice de Gini</span>][r018]*** para los subnodos usando la f√≥rmula de la suma de los cuadrados de probabilidad para √©xito y fracaso $(p^2 + q^2)$. Es decir, calculamos ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Gini</span>][r018]*** para la divisi√≥n usando puntuaci√≥n ***[<span style="font-family:Verdana; font-size:0.95em;color:red">Gini</span>][r018]*** y ponderado para cada nodo de la divisi√≥n.  

  | G√©nero   | Clase    | Altura |
  | :------- | :------- | :----- |
  | Mujer $=>(0,2)^2+(0,8)^2= 0,68$ | IX $=>(0,43)^2+(0,57)^2= 0,51$ | <1,68 $=>(0,42)^2+(0,58)^2= 0,52$ |
  | Hombre $=>(0,65)^2+(0,35)^2= 0,55$ | X $=>(0,56)^2+(0,44)^2= 0,51$ | $\ge1,68=>(0,56)^2+(0,44)^2= 0,50$ |
  | Ponderado $\frac{10}{30}¬∑0.68+\frac{20}{30}¬∑0,55=$ **0,59** | Ponderado $\frac{14}{30}¬∑0.51+\frac{16}{30}¬∑0,51=$ 0,51 | Ponderado $\frac{12}{30}¬∑0.52+\frac{18}{30}¬∑0,50=$ 0,51 |

  **Como el valor m√°s alto es el del g√©nero, este ser√° el nodo ra√≠z.** Recordemos que en el algoritmo ***[<span style="font-family:Verdana; font-size:0.95em;color:red">CART</span>][r015]***, para determinar la ra√≠z, se toman todas las caracter√≠sticas y, para cada una de ellas, se definen todos los posibles umbrales a que haya lugar. Cada umbral ser√° simplemente el punto intermedio entre dos valores consecutivos de cada caracter√≠stica.  

  Adem√°s, para calcular el umbral se puede utilizar la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">ganancia</span>][r012]***, eso quiere decir que para comenzar a dividir, escogemos la impureza m√°s baja, es decir, la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">ganancia</span>][r012]*** mayor. Si eligi√©semos el ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√≠ndice de Gini</span>][r018]***, se coger√≠a el valor m√°s bajo (id√©ntico razonamiento ser√≠a para la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]***).  

  Lo anterior quiere decir que, en un ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]***, cuanto m√°s arriba (hacia la ra√≠z), menor ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** y cuanto m√°s abajo (hacia las hojas), mayor ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** se tiene. Este modo de trabajar permite, para evitar el **overfitting**, **podar** en ramas donde es mayor la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** y mantener ramas en las cuales es menor.

  Calculando la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]***.

  | G√©nero   | Clase    | Altura |
  | :------- | :------- | :----- |
  | Mujer $=>-(\frac{2}{10}log_2(\frac{2}{10}))-(\frac{8}{10}log_2(\frac{8}{10}))= 0,72$| IX $=>-(\frac{6}{14}log_2(\frac{6}{14}))-(\frac{8}{14}log_2(\frac{8}{14}))= 0,99$|$<1,68=>-(\frac{5}{12}log_2(\frac{5}{12}))-(\frac{7}{12}log_2(\frac{7}{12}))= 0,98$|
  | Hombre $=>-(\frac{13}{20}log_2(\frac{13}{20}))-(\frac{7}{20}log_2(\frac{7}{20}))= 0,93$|X $=>-(\frac{9}{16}log_2(\frac{9}{16}))-(\frac{7}{16}log_2(\frac{7}{16}))= 0,99$|$\ge1,68=>-(\frac{10}{18}log_2(\frac{10}{18}))-(\frac{8}{18}log_2(\frac{8}{18}))= 1,55$|
  | Ponderado $=>(\frac{10}{30}¬∑0,72)+(\frac{20}{30}¬∑0,93)=$ **0,83**|Ponderado $=>(\frac{14}{30}¬∑0,99)+(\frac{16}{30}¬∑0,99)=$ 0,99|Ponderado $=>(\frac{12}{30}¬∑0,98)+(\frac{18}{30}¬∑1,55)=$ 1.32|

  Por complemento a 1 con la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]***, podemos calcular la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">ganancia</span>][r012]***.

  | G√©nero   | Clase    | Altura |
  | :------- | :------- | :----- |
  | 1-0.83=**0.17**|1-0.99=0.01|1-1.32=-0.32|

  ***Tal como hab√≠amos supuesto, todo cuadra***, es decir, que ***la menor impureza la tiene el g√©nero***.

  Este m√©todo no solo se aplicar√≠a al **nodo** ra√≠z, sino que se aplicar√≠a recursivamente en todo el  ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]***.

***Nota***: ***Tanto la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** como la ***[<span style="font-family:Verdana; font-size:0.95em;color:red">impureza de Gini</span>][r018]*** son medidas de lo homog√©neo o puro es un conjunto de datos en t√©rminos de las clases que contiene***. La elecci√≥n entre ***usar ***[<span style="font-family:Verdana; font-size:0.95em;color:red">entrop√≠a</span>][r007]*** o ***[<span style="font-family:Verdana; font-size:0.95em;color:red">impureza de Gini</span>][r018]*** a menudo depende del contexto y del conjunto de datos espec√≠fico, pero en la pr√°ctica, ambas m√©tricas suelen llevar a resultados similares en t√©rminos de construcci√≥n de  ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]******.

![Entrop√≠a, M√≠nimo error e √çndice Gini][i023]

#### Utilizaci√≥n

Estos modelos se utilizan com√∫nmente en tareas de **clasificaci√≥n**, como la *detecci√≥n de spam* en el correo electr√≥nico o la **clasificaci√≥n** de clientes en grupos de *segmentaci√≥n de mercado*. Tambi√©n se utilizan en tareas de **regresi√≥n**, como la *predicci√≥n de precios de bienes ra√≠ces*.

***Ejemplo*** 

Supongamos que queremos *decidir si comprar o no un coche usado*. El ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** podr√≠a ser el siguiente:

* ¬øEl coche tiene menos de 5 a√±os?  
    * S√≠: ¬øEl coche tiene menos de 50.000 km?  
        * S√≠: Comprar el coche.  
        * No: No comprar el coche.  
    * No: ¬øEl coche tiene menos de 80.000 km?  
        * S√≠: Comprar el coche.  
        * No: No comprar el coche.  

Este **√°rbol** tiene dos **nodos**, con cada **nodo** representando una pregunta y cada **rama** representando una posible respuesta. En funci√≥n de las respuestas a las preguntas, se llega a una **hoja**, la cual *indica si se debe comprar o no el coche usado*.

## Pasos para implementar √°rboles de decisi√≥n  

### ***1) Recopilaci√≥n y preparaci√≥n de datos***  

* Recolectar los *datos relevantes* para el proyecto.  
* Realizar una *limpieza y preprocesamiento de los datos*, como, por ejemplo, *eliminar **outliers*** (valores at√≠picos) y *manejar datos faltantes*.  
* *Dividir los datos en conjuntos de entrenamiento y prueba (**validaci√≥n cruzada**)* para *evaluar el rendimiento del modelo*.  

### ***2) Selecci√≥n de caracter√≠sticas***  

* *Identificar las caracter√≠sticas m√°s relevantes* para el modelo.  
* Utilizar *t√©cnicas de selecci√≥n de caracter√≠sticas*, como la correlaci√≥n y la importancia de las variables.  

### ***3) Entrenamiento del modelo***  

* Utilizar un *algoritmo* de ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]***, como el ***[<span style="font-family:Verdana; font-size:0.95em;color:red">CART</span>][r015]*** o el ***[<span style="font-family:Verdana; font-size:0.95em;color:red">C4.5</span>][r010]***, *para entrenar el modelo*.  
* Asegurarse de *ajustar los **hiperpar√°metros*** del algoritmo *para obtener el mejor rendimiento*.  

### ***4) Validaci√≥n del modelo***  

* Evaluar el *rendimiento del modelo utilizando **m√©tricas*** como **la precisi√≥n, la exhaustividad y el valor F**.  
* Utilizar *t√©cnicas de validaci√≥n cruzada*, como la **validaci√≥n cruzada k-fold**, *para obtener una estimaci√≥n m√°s precisa del rendimiento del modelo*.  

### ***5) Ajuste y optimizaci√≥n del modelo***  

* *Si el rendimiento del modelo no es satisfactorio, realizar ajustes en los par√°metros del modelo o probar diferentes algoritmos* de ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]***.  
* Utilizar *t√©cnicas de optimizaci√≥n*, como la **poda**, *para mejorar la generalizaci√≥n del modelo*.  

### ***6) Implementaci√≥n y despliegue***  

* *Una vez que el modelo ha sido entrenado y validado, se puede implementar en un entorno de producci√≥n*.  
* *Integrar el modelo en una aplicaci√≥n o sistema existente para utilizarlo en la toma de decisiones*.  

### ***7) Adicionalmente***  

#### ***7.1) Seleccionar el algoritmo adecuado***

Existen diferentes tipos de algoritmos de ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]***, como ***[<span style="font-family:Verdana; font-size:0.95em;color:red">CART</span>][r015], [<span style="font-family:Verdana; font-size:0.95em;color:red">C4.5</span>][r010] y Random Forest***. Es importante *seleccionar el algoritmo adecuado seg√∫n las caracter√≠sticas y los requisitos del proyecto*.  

#### ***7.2) Considerar la interpretabilidad del modelo*** 

Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** *son modelos f√°cilmente interpretables*, lo que significa que se puede entender c√≥mo se toman las decisiones. Esto puede ser especialmente √∫til en proyectos que requieren explicabilidad, como pueda ser el sector financiero.  

#### ***7.3) Realizar una validaci√≥n exhaustiva***

Es importante *realizar diferentes pruebas y validaciones del modelo para garantizar que est√© funcionando correctamente*. Adem√°s de las ***m√©tricas** de rendimiento*, se pueden utilizar t√©cnicas como la **matriz de confusi√≥n** y la **curva ROC** para evaluar el rendimiento del modelo en diferentes escenarios.  

#### ***7.4) Evaluar la estabilidad del modelo***

Algunos ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** *pueden ser sensibles a cambios peque√±os en los datos de entrada, lo que puede afectar el rendimiento del modelo*. Es importante *evaluar la estabilidad del modelo y realizar pruebas con diferentes conjuntos de datos para asegurarse de que sea robusto*.

> Fuente: [<span style="font-family:Verdana; font-size:0.95em;color:red">*Gu√≠a completa sobre algoritmos de √°rboles de decisi√≥n en Espa√±a*</span>][r011] 

### Matriz de confusi√≥n

En el campo del ML una **matriz de confusi√≥n** es una herramienta que permite visualizar el desempe√±o de un algoritmo  de **aprendizaje supervisado**. *Cada columna de la matriz representa el n√∫mero de predicciones de cada **clase***, mientras que cada *fila representa a las instancias en la **clase** real*, o sea, en t√©rminos pr√°cticos nos ***permite ver qu√© tipos de aciertos y errores est√° teniendo nuestro modelo*** a la hora de pasar por el proceso de aprendizaje con los datos.

Resultados posibles:

* ***Verdadero positivo (VP)***: El valor real es positivo y  la prueba predijo tambien que era positivo.  
En el √°rea de la salud: ***El paciente tiene la enfermedad y el test es positivo***.  
* ***Verdadero negativo (VN)***: El valor real  es negativo y la prueba predijo tambien que el resultado era negativo.  
En el √°rea de la salud: ***El paciente no tiene la enfermedad y el test es negativo***.  
* ***Falso negativo (FN)***: El valor real es positivo, y la prueba predijo  que el resultado es negativo. Esto es lo que en Estad√≠stica se conoce como ***error tipo II***.  
En el √°rea de la salud: ***El paciente tiene la enfermedad pero el resultado del test es negativo***.  
* ***Falso positivo (FP)***: El valor real es negativo, y la prueba predijo  que el resultado es positivo. Esto es lo que en Estad√≠stica se conoce como ***error tipo I***.  
En el √°rea de la salud: ***El paciente no tiene la enfermedad pero el resultado del test es positivo***.

A partir de estas 4 opciones  surgen las ***m√©tricas** de la **matriz de confusi√≥n***:  Por una parte la **accuracy (exactitud)** y la **precision (precisi√≥n)** y por otra la **recall (sensibilidad)** y la **especificity (especificidad)**.  

![Matriz de confusi√≥n][i024]  

#### M√©tricas

##### ***Accuracy (exactitud)***  

Se refiere a ***lo cerca que est√° el resultado de una medici√≥n del valor verdadero***. En t√©rminos estad√≠sticos, la exactitud est√° relacionada con el **bias (sesgo)** de una estimaci√≥n. Se representa como la proporci√≥n de resultados verdaderos dividido entre el n√∫mero total de casos examinados. En forma pr√°ctica, ***la exactitud es la cantidad de predicciones acertadas***.

Accurancy $=\frac{VP+VN}{VP+FP+FN+VN}$  

![Accuracy][i025]

##### ***Precision (precisi√≥n)***  

Se refiere a la ***dispersi√≥n del conjunto de valores obtenidos a partir de mediciones repetidas de una magnitud. Cuanto menor es la dispersi√≥n mayor la precisi√≥n***. Se representa por la proporci√≥n de verdaderos positivos dividido entre todos los resultados positivos. En forma pr√°ctica, ***la precisi√≥n es el porcentaje predicciones positivas acertadas***.

Precision $=\frac{Resultados\_positivos}{Total\_resultados\_positivos}=\frac{VP}{VP+FP}$  

![Precision][i026]

##### ***Bias, √≥ inaccuracy (sesgo)***

***Es la diferencia entre el valor medio y el verdadero valor de la magnitud medida***. El **sesgo (bias)** pertenece al concepto de **exactitud**.  

##### ***Recall o sensitivity (sensibilidad)***  

Tambi√©n se conoce como **Tasa de verdaderos positivos (true positive rate o TP)**. En forma pr√°ctica, ***la sensibilidad es el porcentaje de casos positivos que fueron correctamente identificados***.

Sensitivity $=\frac{VP}{VP+FN}$

***En el √°rea de la salud***:  $=\frac{Verdaderos\_positivos}{Total\_enfermos}$, es dicir, es ***la capacidad de de poder detectar correctamente la enfermedad entre los enfermos***.  

![Recall][i027]

##### ***Especificity (especificidad)***  

Tambi√©n conocida como la **Tasa de verdaderos negativos, (true negative rate o TN)**. En forma pr√°ctica, ***la Especificidad es el porcentaje de casos negativos que fueron correctamente identificados***. Expresa como de bien puede el modelo detectar esa **clase**.

Especificity $=\frac{VN}{VN+FP}$  

***En el √°rea de la salud***:  $=\frac{Verdaderos\_negativos}{Total\_sanos}$, es dicir, es ***la capacidad de poder identificar los casos de pacientes sanos entre todos los sanos***.  

![Especificity][i028]  

##### ***Relaci√≥n entre recall (sensibilidad) y especificity (especificidad)***  

La **sensibilidad** y la **especificidad** son dos valores que nos *indican la capacidad de nuestro estimador para discriminar los casos positivos, de los negativos*. La **sensibilidad** se representa como la *fracci√≥n de verdaderos positivos*, mientras que la **especificidad**, es la *fracci√≥n de verdaderos negativos*.

![M√©tricas de la matriz de confusi√≥n][i033] 

##### ***Mediciones de la precisi√≥n de una prueba***  

Al calcular las relaciones entre estos valores, podemos medir cuantitativamente la **precisi√≥n de nuestras pruebas**.  

###### ***Tasa de falsos positivos (TFP)***

La **tasa de falsos positivos o TFP** es la probabilidad de que se produzca una ***falsa alarma***, es decir, que se d√© un resultado positivo cuando el valor verdadero sea negativo. 

En el √°rea de la salud: Imaginemos que *diagnostiquemos como enfermo a una persona sana*, est√° calro que ***no es un error muy grave (es una falsa alarma)***, porque un segundo diagn√≥stico, probablemente lo identifique como sano. 

TFP $=\frac{Falsos\_positivos}{Total\_negativos}=\frac{FP}{FP+VN}$ 
donde  
FP = **n√∫mero de falsos positivos**
VN = **n√∫mero de verdaderos negativos**
FP+VN=**n√∫mero total de casos negativos**

![Tasa de falsos positivos][i029]

###### ***Tasa de falsos negativos (TFN o tasa de error)***

La **tasa de falsos negativos (TFN o tasa de error)**, es la ***probabilidad de que la prueba pase por alto un verdadero positivo***. 

En el √°rea de la salud: Imaginemos que *diagnostiquemos como sano a una persona enferma*, est√° calro que ***es un error muy grave***, porque no habr√° un segundo diagn√≥stico para una persona enferma. En el caso de una persona con c√°ncer, probablemente la condenemos a muerte. 

TFN $=\frac{Falsos\_negativos}{Total\_positivos}=\frac{FN}{FN+VP}$
donde  
FN = **n√∫mero de falsos negativos**
VP = **n√∫mero de verdaderos positivos**
FN+VP=**n√∫mero total de casos positivos**  

![Tasa de falsos negativos][i032]

###### ***Tasa de verdadero positivos (TVP o sensibilidad)***

La **tasa de verdadero positivos (TVP) es la sensibilidad** y ***es la probabilidad de que un resultado positivo real d√© positivo***. Dicho de otro modo, la **sensibilidad** de nuestro modelo.  

###### ***Tasa de verdaderos negativos (TVN o especificidad)***

La **tasa de verdaderos negativos (TVN) es la especificidad** y ***es la probabilidad de que un resultado negativo real d√© un resultado negativo***. Dicho de otro modo, lo **espec√≠fico** de nuestro modelo.  

  * El **valor predictivo positivo (VP+)** ***es la probabilidad de que, si ha obtenido un resultado positivo en la prueba, realmente sea cierto***.  
  VP+$=\frac{Positivos\_reales}{Total\_positivos\_en\_pruebas}=\frac{VP}{VP+FP}$
  En el √°rea de la salud: La probabilidad de tener la enfermedad si el resultado de la prueba diagn√≥stica es positivo.

  * El **valor predictivo negativo (VP-)**, por el contrario ***es la probabilidad de que, si ha obtenido un resultado negativo en la prueba, en realidad sea falso***.  
  VP-$=\frac{Negativos\_reales}{Total\_negativos\_en\_pruebas}=\frac{VN}{VN+FN}$
  En el √°rea de la salud: La probabilidad de no tener la enfermedad si el resultado de la prueba diagn√≥stica es negativo.
  
  * Los **valores predictivos (positivo y negativo)** miden la eficacia real de una prueba diagn√≥stica. Son probabilidades del resultado, es decir, dan la probabilidad de padecer o no una enfermedad una vez conocido el resultado de la prueba diagn√≥stica. Se trata de valores post-test y dependen de la prevalencia de una enfermedad, es decir, en el √°rea de la salud, el porcentaje de una poblaci√≥n que est√° afectada por esa determinada patolog√≠a.

###### ***F1 Score***  

Esta es otra m√©trica muy empleada porque nos ***resume la precisi√≥n y sensibilidad en una sola m√©trica***. Por ello ***es de gran utilidad cuando la distribuci√≥n de las clases es desigual***, lo que acostumbra a ser bastante com√∫n.  

bullshit

  $$
  \begin{align*}
  F1 Score &=\frac{2}{Precisi√≥n^{-1}+Sensibilidad^{-1}}\\  
    &=\frac{2¬∑Precisi√≥n¬∑Sensibilidad}{Precisi√≥n+Sensibilidad}\\
    &=\frac{2¬∑(\frac{VP}{VP+FP}¬∑\frac{VP}{VP+FN})}{\frac{VP}{VP+FP}+\frac{VP}{VP+FN}}\\
    &=\frac{2VP}{2VP+FP+FN}\\
  \end{align*}
  $$  

![F1 Score][i030]

##### ***Resumen***  
 
Podemos obtener cuatro casos posibles para cada **clase**:  

1. ***Alta precisi√≥n y alta sensibilidad***: El ***modelo*** de ML escogido ***maneja perfectamente esa **clase*****.  
2. ***Alta precisi√≥n y baja sensibilidad***: El ***modelo*** de ML escogido ***no detecta la **clase** muy bien, pero cuando lo hace es altamente confiable***.  
3. ***Baja precisi√≥n y alta sensibilidad***: El ***modelo*** de ML escogido ***detecta bien la **clase**, pero tambi√©n incluye muestras de la otra **clase*****.  
4. ***Baja precisi√≥n y baja sensibilidad***: El ***modelo*** de ML escogido ***no logra clasificar la **clase** correctamente***.

Cuando tenemos un ***‚Äúdataset‚Äù  con desequilibrio, suele ocurrir que obtenemos un alto valor de **precisi√≥n** en la **clase** mayoritaria y un bajo **recall** en la **clase** minoritaria***.  Esta circunstancia es particularmente ****frecuente* y por ello tenemos que recurrir al **balanceo de clases*****.  

![Resumen de m√©tricas][i031]

#### ***Consejos generales sobre la matriz de confusi√≥n y sus m√©tricas***  

1) La **precisi√≥n** es un gran valor estad√≠stico, pero *es √∫til √∫nicamente cuando se tienen ‚Äúdatasets‚Äù sim√©tricos* (la cantidad de casos de la clase 1 y de las clase 2 tienen magnitudes similares).

2) El **indicador F1 Score** de la **matriz de confusi√≥n** *es √∫til si se  tiene una distribuci√≥n de clases desigual*, ***por ejemplo cuando el n√∫mero de pacientes con una condici√≥n es del 15% y el otro es 85%, lo que en el campo de la salud es bastante com√∫n***.

3) Debemos elegir mayor **precisi√≥n** *para conocer qu√© tan seguro estamos de los verdaderos positivos*, mientras que la ***sensibilidad o  ‚ÄúRecall‚Äù** nos servir√° para saber si no estamos perdiendo positivos*.  

4) Las **Falsas Alarmas**, como por ejemplo, *si creemos que es mejor en nuestro caso tener falsos positivos que falsos negativos, debemos utilizar una sensibilidad alta (Recall), cuando la aparici√≥n de falsos negativos nos resulta inaceptable pero no nos importa tener falsos positivos adicionales (falsas alarmas).*  
***Ejemplo***: ***Preferimos que algunas personas sanas sean etiquetadas como diab√©ticas en lugar de dejar a una persona diab√©tica etiquetada como sana***.

5) Debemos elegir **precisi√≥n** *si queremos estar m√°s seguros de nuestros verdaderos positivos, por ejemplo, correos electr√≥nicos no deseados.*  
***Ejemplo***: ***Se prefiere tener algunos correos electr√≥nicos ‚Äúno deseados‚Äù en su bandeja de entrada en lugar de tener correos electr√≥nicos ‚Äúreales‚Äù en su bandeja de SPAM***.

6) Debemos elegir **alta Especificidad**, *si deseamos identificar los verdaderos negativos, o lo que es igual cuando no deseamos falsos positivos.*   
***Ejemplo***: ***Se est√° llevando a cabo una prueba de drogas en la que todas las personas que dan positivo ir√°n a la c√°rcel de inmediato, la idea es que ninguna persona ‚Äúlibre de drogas‚Äù vaya a la c√°rcel***. Los falsos positivos aqu√≠ son intolerables.

> Fuente: [<span style="font-family:Verdana; font-size:0.95em;color:red">*La matriz de confusi√≥n y sus m√©tricas*</span>][r014]

#### ***Balanceo de clases***

***Clases desbalanceadas en modelos de ML (Imbalanced data)***  

Cuando utilizamos modelos de ML para fines de **clasificaci√≥n**, corremos el riesgo de encontrarnos con **clases desbalanceadas (imbalanced data)** . Por ejemplo cuando tenemos que etiquetar  ¬´spam¬ª o ¬´no spam¬ª √≥ entre m√∫ltiples categor√≠as (Medicamento 1, 2 o 3) *es frecuente encontrar que en nuestro conjunto de datos de entrenamiento contamos con que alguna de las **clases** de la muestra es una **clase** ¬´minoritaria¬ª es decir, de la cual tenemos muy pocos casos*. Esto provoca un desbalanceo en los datos que utilizaremos para el entrenamiento de nuestra m√°quina. Adem√°s el problema es que al final de cuentas, *nuestro algoritmo ser√° muy bueno detectando las **clases** mayoritarias, pero muy malo detctando las minoritarias, lo que nos puede acarrerar muchos errores tipo 1, (Falsos Positivos)*.  

Un caso muy evidente es en el √°rea de Salud en donde solemos encontrar conjuntos de datos con miles de registros con pacientes ¬´negativos¬ª y unos pocos casos positivos es decir, que padecen la enfermedad que queremos clasificar.

Otros ejemplos suelen ser los de detecci√≥n de fraude en las tarjetas de cr√©dito, donde tenemos muchas muestras de clientes ¬´honestos¬ª y pocos casos etiquetados como fraudulentos.

***¬øC√≥mo nos afectan las **Clases** desbalanceadas en modelos de Machine Learning?***  

Por lo general nos afecta en su proceso de ‚Äúgeneralizaci√≥n de la informaci√≥n‚Äù (aplicar el conocimiento a casos nuevos) y perjudicando a las **clases** minoritarias. Esto suena bastante razonable: si a una red neuronal le damos 990 de fotos de gatitos y s√≥lo 10 de perros, no podemos pretender que logre diferenciar una clase de otra. Lo m√°s probable que la red se limite a responder siempre ¬´tu foto es un gato¬ª puesto que as√≠ tuvo un acierto del 99% en su fase de entrenamiento.  

***Estrategias para el manejo de **Clases**  Desbalanceadas***  

* ***Ajuste de Par√°metros del modelo***  
Consiste en ajustar **parametros** √≥ **metricas** del propio algoritmo para intentar *equilibrar a la **clase** minoritaria, penalizando a la **clase** mayoritaria durante el entrenamiento.*  
Ejemplos: Ajuste de peso en **√°rboles**, tambi√©n en **logistic regression** tenemos el **par√°metro class_weight= ¬´balanced¬ª**.  

* ***Modificar el Dataset***  
Por ejemplo, podemos *eliminar muestras de la **clase** mayoritaria para reducirlo e intentar equilibrar la situaci√≥n. Tiene de ¬´peligroso¬ª que podemos prescindir de muestras importantes, que brindan informaci√≥n y por lo tanto empeorar el modelo.* Entonces para seleccionar qu√© muestras eliminar, deber√≠amos seguir alg√∫n criterio. *Tambi√©n podr√≠amos agregar nuevas filas con los mismos valores de las clases minoritarias.* Pero esto no sirve demasiado y *podemos llevar al modelo a caer en **overfitting**.*

* ***Muestras artificiales***  
Primero *podemos intentar crear muestras sint√©ticas (no id√©nticas) utilizando diversos algoritmos que intentan seguir la tendencia del grupo minoritario.* Seg√∫n el m√©todo, podemos mejorar los resultados. *Lo peligroso de crear muestras sint√©ticas es que podemos alterar la distribuci√≥n ¬´natural¬ª de esa **clase** y confundir al modelo en su clasificaci√≥n.*

* ***Balanced Ensemble Methods***  
Utilizar las ventajas de hacer **ensamble de m√©todos**, es decir, *entrenar diversos modelos y entre todos obtener el resultado final (por ejemplo ¬´votando¬ª), pero se asegura de tomar muestras de entrenamiento equilibradas.*  

***Conclusiones***  

*Es muy frecuente encontrarnos ***clases desbalanceadas*** en modelos de ML, de hecho, lo m√°s raro ser√≠a encontrar datasets bien equilibrados.*

Una pregunta frecuente es *¬´¬øQue hacer cuando tengo pocas muestras de una **clase**?*¬ª. La primera respuesta y la de sentido com√∫n es *¬´Salir a la calle y conseguir m√°s muestras!¬ª* pero la realidad es que no siempre es posible conseguir m√°s datos de las **clases** minoritarias (como por ejemplo en Casos de Salud).

Como resultado la **Matriz de Confusi√≥n** es un √∫til instrumento para medir si las m√©tricas pueden ser enga√±osas. *Si miramos a nuestros aciertos √∫nicamente, puede que pensemos que tenemos un buen clasificador, cuando realmente est√° fallando.*

> Fuente: [<span style="font-family:Verdana; font-size:0.95em;color:red">*Clases desbalanceadas en modelos de Machine Learning*</span>][r016] 

#### ***Hiperpar√°metros***

Los modelos de ML ya no son modelos lineales. Utilizan **‚Äúhiperpar√°metros‚Äù** para mejorar el rendimiento de los algoritmos, realizar selecci√≥n de variables de entrada (regularizaci√≥n), evitar el **sobreajuste** y optimizar el procesamiento.  

Los **hiperpar√°metros** no est√°n en los datos de entrenamiento y no se conocen a priori. Una mejor selecci√≥n de sus valores se hace mediante un proceso de ajuste de par√°metros o tuning en funci√≥n de resultados de validaci√≥n.  

Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** son flujogramas que mapean las posibles salidas del objetivo que buscamos en funci√≥n de una serie de elecciones o datos de entrada relacionados.  

Por lo general, un ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** comienza con un solo **nodo** o **nodo ra√≠z**, que se ramifica seg√∫n preguntas que nos hacemos sobre los atributos de entrada (**nodos internos**). El ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]*** se va formando de acuerdo a los resultados de dichas preguntas hasta los **√∫ltimos nodos (hojas)** que son los que incluyen la **clasificaci√≥n**, etiqueta o n√∫mero buscado.  

Es un algoritmo **supervisado** y los principales **hiperpar√°metros** de este tipo de modelo son:
1. **La profundidad m√°xima del ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]*****: El n√∫mero de saltos, cortes o test sobre atributos.
2. **El m√≠nimo n√∫mero de muestras por hoja** o nodo final

Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** son f√°ciles de construir y de interpretar y de escalar sin embargo, son bastante inexactos, funcionan bien con los valores con los que se han entrenado pero no tanto con nuevos valores (alta varianza).  

> Fuente: [<span style="font-family:Verdana; font-size:0.95em;color:red">*El superpoder de los hiperpar√°metros*</span>][r017]

## ¬øC√≥mo funcionan los √°rboles de decisi√≥n?

Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** son un tipo de modelo de **aprendizaje supervisado** que se utilizan para **clasificar o predecir valores num√©ricos**. Toman un conjunto de datos de entrada, lo dividen en subconjuntos basados en diferentes atributos y, finalmente, hacen una predicci√≥n sobre el valor de salida.

Por ejemplo, consideremos el siguiente conjunto de datos que describe varios productos que se venden en una tienda en l√≠nea:  

|Producto | Precio | Descuento | En Oferta |
| ------- | -----: | --------: | --------- |  
| A | 10 | 0.05 | S√≠ |
| B | 20 | 0.10 | No |
| C | 30 | 0.20 | S√≠ |
| D | 15 | 0.15 | S√≠ |
| E | 25 | 0.10 | No |

En este caso, queremos construir un ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** que nos permita **predecir si un producto estar√° en oferta o no, en funci√≥n de sus caracter√≠sticas**.

Comenzamos por construir el **nodo ra√≠z**, que representa el conjunto completo de datos. A continuaci√≥n, seleccionamos un atributo para dividir el conjunto de datos. En este caso, podr√≠amos elegir el atributo **"Precio"** como nuestra primera divisi√≥n.

Supongamos que elegimos un umbral de 20‚Ç¨ para el precio, lo que significa que cualquier producto con un precio mayor a 20‚Ç¨ se considerar√° "caro" y cualquier producto con un precio menor o igual a 20‚Ç¨ se considerar√° "barato". La primera divisi√≥n del ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]*** quedar√≠a as√≠:

````
                     Precio <= 20
                    /            \
         S√≠ (Producto Barato)  No (Producto Caro) 
````

El ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** tiene dos ramas: una **rama** para los productos baratos y otra para los productos caros. La **rama** para los productos baratos conduce a un **nodo hoja** que indica que el producto A est√° en oferta. La **rama** para los productos caros conduce a otro **nodo de decisi√≥n**, que utiliza el descuento para decidir si el producto est√° en oferta o no.

Supongamos que elegimos un umbral de descuento del 15%. Entonces, el ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** completo quedar√≠a as√≠:

````
                     Precio <= 20
                    /            \
         S√≠ (Producto Barato)  No (Producto Caro) 
                  /                       \
      (Descuento <= 3%)              (Descuento <= 15%)
      /            \                  /                \
S√≠ (Sin oferta)  No (En oferta)    S√≠ (Sin oferta)  No (En oferta) 
```` 
Este ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]*** nos permite predecir si un producto est√° en oferta o no, en funci√≥n de su precio y de su descuento.

### ***Ventajas***

* **Algoritmo de caja blanca**: los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** son considerados algoritmos de **caja blanca**, lo que significa que son modelos f√°cilmente interpretables y comprensibles por los humanos.

* **Resultados f√°ciles de interpretar y entender**: los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** son modelos f√°ciles de interpretar y entender, ya que cada paso en el proceso de toma de decisiones se representa expl√≠citamente. Esto permite que los expertos del dominio puedan validar el modelo y dar sugerencias para mejorarlo.

* **Las combinaciones de los mismos pueden dar resultados muy certeros**: los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** individuales pueden ser limitados en t√©rminos de precisi√≥n de predicci√≥n. Sin embargo, una t√©cnica que se utiliza para mejorar la precisi√≥n de los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** es la combinaci√≥n de varios ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles</span>][r000]*** en un conjunto, como en el caso de **Random Forest**. Esto se conoce como **ensamblaje de modelos** y puede proporcionar una mayor precisi√≥n en las predicciones.

### ***Desventajas***

* **Tendencia al overfitting**: Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** tienen una tendencia natural al **overfitting**, lo que significa que pueden ajustarse demasiado a los datos de entrenamiento y no generalizar bien para nuevos datos. Esto se puede prevenir mediante t√©cnicas de **poda** o regularizaci√≥n.

* **Influencia de los outliers**: Los **outliers** pueden tener una influencia significativa en la creaci√≥n de los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]***, ya que pueden **sesgar** la partici√≥n de los datos. Una soluci√≥n a esto es utilizar t√©cnicas de preprocesamiento de datos para tratar con los **outliers** antes de crear el ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]***.

* *****[<span style="font-family:Verdana; font-size:0.95em;color:red">√Årboles</span>][r000]*** demasiado complejos pueden no adaptarse bien a los nuevos datos**: Si se crean ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** demasiado complejos, pueden adaptarse demasiado a los datos de entrenamiento y no generalizar bien para nuevos datos. Esto se puede prevenir mediante t√©cnicas de **pruning** o regularizaci√≥n para simplificar el ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]***.

* **Posibilidad de crear ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles</span>][r000]*** sesgados si una clase es m√°s numerosa**: si una **clase** es significativamente m√°s numerosa que las dem√°s **clases**, el ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol de decisi√≥n</span>][r000]*** puede estar sesgado hacia esa **clase** mayoritaria y no prestar suficiente atenci√≥n a las otras **clases**. Esto se puede prevenir mediante t√©cnicas de **balanceo de clases**, como el muestreo estratificado o el aumento de datos.

### ***¬øCu√°ndo usar √°rboles de decisi√≥n?***  

* **Sencillo y f√°cil de entender**: Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** son una t√©cnica sencilla y f√°cil de interpretar. Por lo tanto, ***es recomendable utilizarlos cuando se busca una soluci√≥n clara y f√°cil de entender***. Adem√°s, la estructura de ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rbol</span>][r000]*** es intuitiva y f√°cil de visualizar, lo que hace que el proceso de toma de decisiones sea m√°s f√°cil de entender.

* **Funcionan bastante bien con grandes conjuntos de datos**: Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** pueden funcionar muy bien con grandes conjuntos de datos. ***A medida que el tama√±o del conjunto de datos aumenta, los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** se vuelven m√°s √∫tiles***, ya que pueden segmentar el conjunto de datos en grupos m√°s peque√±os y m√°s manejables para un an√°lisis m√°s profundo.

* **Relativamente robusto**: Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** son relativamente robustos y ***pueden manejar datos faltantes o ruidosos***. Adem√°s, ***son √∫tiles en situaciones en las que se necesitan resultados precisos con una alta tasa de precisi√≥n***.

* **M√©todo √∫til para analizar datos cuantitativos**: Los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** son particularmente √∫tiles para analizar datos cuantitativos. Por ejemplo, en el an√°lisis de negocios, los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** ***pueden ayudar a identificar patrones en grandes conjuntos de datos y hacer recomendaciones basadas en esa informaci√≥n***.

* **Aplicable para clasificaci√≥n y regresi√≥n**: los ***[<span style="font-family:Verdana; font-size:0.95em;color:red">√°rboles de decisi√≥n</span>][r000]*** se pueden utilizar para problemas de **clasificaci√≥n** y **regresi√≥n**. ***En problemas de **clasificaci√≥n**, se utiliza para asignar una etiqueta a un objeto***, mientras que ***en problemas de **regresi√≥n**, se utiliza para predecir una variable continua***.  

## Listado de referencias externas

* √Årboles de decisi√≥n (Wikipedia)  

[r000]: https://es.wikipedia.org/wiki/%C3%81rbol_de_decisi%C3%B3n "referencia a √°rboles de decisi√≥n en Wikipedia"

* Machine Learning (Wikipedia)  

[r001]: https://es.wikipedia.org/wiki/Aprendizaje_autom%C3%A1tico "referencia a Machine Learning en Wikipedia"

* Aprendizaje supervisado (Wikipedia) 

[r002]: https://es.wikipedia.org/wiki/Aprendizaje_supervisado "referencia a Aprendizaje supervisado en Wikipedia"

* Leo Breiman (Wikipedia)  

[r003]: https://es.wikipedia.org/wiki/Leo_Breiman "referencia a Leo Breiman en Wikipedia"

* Decision Trees y Random Forest con Python y scikit-learn  (Carlos Mazzaroli)  

[r004]: https://deepnote.com/app/mazzaroli/Decision-Trees-y-Random-Forest-con-Python-y-scikit-learn-22e03409-93bd-4c7e-9ade-f94470cd6941 "referencia a Decision Trees y Random Forest con Python y scikit-learn  (Carlos Mazzaroli)"

* Clasificaci√≥n estad√≠stica (Wikipedia)  

[r005]: https://es.wikipedia.org/wiki/Clasificaci%C3%B3n_estad%C3%ADstica "referencia a Clasificaci√≥n estad√≠stica en Wikipedia"

* An√°lisis de la regresi√≥n (Wikipedia)  

[r006]: https://es.wikipedia.org/wiki/An%C3%A1lisis_de_la_regresi%C3%B3n "referencia a An√°lisis de la regresi√≥n en Wikipedia"

* Entrop√≠a (informaci√≥n) (Wikipedia)  

[r007]: https://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n) "referencia a Entrop√≠a (informaci√≥n) en Wikipedia"

* ID3 (IBM)  

[r008]: https://www.ibm.com/es-es/topics/decision-trees "referencia a ID3 en IBM"  

* Ross Quinlan (Wikipedia)  

[r009]: https://es.wikipedia.org/wiki/Ross_Quinlan "referencia a Ross Quinlan en Wikipedia"

* C4.5 (IBM)  

[r010]: https://www.ibm.com/es-es/topics/decision-trees "referencia a C4.5 en IBM"  

* Gu√≠a completa sobre algoritmos de √°rboles de decisi√≥n en Espa√±a  

[r011]: https://prompt.uno/aprendizaje-automatico/algoritmos-de-arboles-de-decision/ "referencia a Gu√≠a completa sobre algoritmos de √°rboles de decisi√≥n en Espa√±a"
 
* C√≥mo elegir el mejor atributo en cada nodo (IBM)  

[r012]: https://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n) "referencia a C√≥mo elegir el mejor atributo en cada nodo en IBM"

* Tipos de √°rboles de decisi√≥n (IBM)  

[r013]: https://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n) "referencia a Tipos de √°rboles de decisi√≥n en IBM"
 
* La matriz de confusi√≥n y sus m√©tricas  

[r014]: https://www.juanbarrios.com/la-matriz-de-confusion-y-sus-metricas/ "referencia a La matriz de confusi√≥n y sus m√©tricas"

* Clasificaci√≥n con √Årboles de Decisi√≥n: el algoritmo CART  

[r015]: https://www.codificandobits.com/blog/clasificacion-arboles-decision-algoritmo-cart/ "referencia a Clasificaci√≥n con √Årboles de Decisi√≥n: el algoritmo CART"

* Clases desbalanceadas en modelos de Machine Learning  

[r016]: https://www.juanbarrios.com/clases-desbalanceadas/ "referencia a Clases desbalanceadas en modelos de Machine Learning"

* El superpoder de los hiperpar√°metros  

[r017]: https://impulsatek.com/8-el-superpoder-de-los-hiperparametros/ "referencia a El superpoder de los hiperpar√°metros"

* √çndice (o impureza) de Gini (Wikipedia)  

[r018]: https://es.wikipedia.org/wiki/Aprendizaje_basado_en_%C3%A1rboles_de_decisi%C3%B3n#Impureza_de_Gini "referencia a √çndice (o impureza) de Gini en Wikipedia"

## Listado de im√°genes

* Leo Breiman  

[i000]: https://i.imgur.com/SszQ8rp.jpg "Leo Breiman"

* Aprendizaje supervisado  

[i001]: https://i.imgur.com/oOJqpy5.png "Aprendizaje supervisado" 

* Regresi√≥n vs clasificaci√≥n 001  

[i002]: https://i.imgur.com/XlIxQhK.png "Regresi√≥n vs clasificaci√≥n 001"

* Regresi√≥n vs clasificaci√≥n 002  

[i003]: https://i.imgur.com/q1aJfN2.png "Regresi√≥n vs clasificaci√≥n 002"

* Regresi√≥n vs clasificaci√≥n 003  

[i004]: https://i.imgur.com/lIWGXgj.png "Regresi√≥n vs clasificaci√≥n 003"

* Nodo  

[i005]: https://i.imgur.com/FjY24Nw.png "Nodo"  

* Nodo ra√≠z  

[i006]: https://i.imgur.com/OUJejNh.png "Nodo ra√≠z"  

* Rama  

[i007]: https://i.imgur.com/pUoPeUR.png "Rama"  

* Nodo de decisi√≥n  

[i008]: https://i.imgur.com/LVqwGAy.png "Nodo de decisi√≥n"  

* Divisi√≥n  

[i009]: https://i.imgur.com/5fslA9D.png "Divisi√≥n"  

* Nodo padre  

[i010]: https://i.imgur.com/WUGhyKD.png "Nodo padre"  

* Nodo hijo  

[i011]: https://i.imgur.com/aadGRYk.png "Nodo hijo"  

* Nodo hoja  

[i012]: https://i.imgur.com/Ps9f7Z8.png "Nodo hoja"  

* Overfitting  

[i013]: https://i.imgur.com/sMamasP.png "Overfitting" 

* Pruning  

[i014]: https://i.imgur.com/yKRally.png "Pruning" 

* Vectores  

[i015]: https://i.imgur.com/GxPsEFi.png "Vectores" 

* Flechas  

[i016]: https://i.imgur.com/BXdrEvn.png "Flechas" 

* Etiquetas  

[i017]: https://i.imgur.com/W3VVLyT.png "Etiquetas" 

* Clase  

[i018]: https://i.imgur.com/pKoyDIW.png "Clase" 

* Validaci√≥n cruzada  

[i019]: https://i.imgur.com/42FaO1M.png "Validaci√≥n cruzada" 

* Outliers  

[i020]: https://i.imgur.com/kQmu8fm.png "Outliers" 

* Entrop√≠a de la informaci√≥n  

[i021]: https://i.imgur.com/hUiUKay.png "Entrop√≠a de la informaci√≥n" 

* Ejemplo estudiantes  

[i022]: https://i.imgur.com/AwM84vI.png "Ejemplo estudiantes"

* Entrop√≠a, M√≠nimo error e √çndice Gini  

[i023]: https://i.imgur.com/n1az3GF.png "Entrop√≠a, M√≠nimo error e √çndice Gini"

* Matriz de confusi√≥n  

[i024]: https://i.imgur.com/a8Vl1dF.png "Matriz de confusi√≥n"

* Accuracy  

[i025]: https://i.imgur.com/rJ5d6rF.png "Accuracy"

* Precision  

[i026]: https://i.imgur.com/Ev3aWMB.png "Precision"

* Recall  

[i027]: https://i.imgur.com/ZX1ZMw7.png "Recall"

* Especificity  

[i028]: https://i.imgur.com/N9RdMWG.png "Especificity"

* Tasa de falsos positivos  

[i029]: https://i.imgur.com/keI2Lxg.png "Tasa de falsos positivos"

* F1 Score  

[i030]: https://i.imgur.com/erPJC6p.png "F1 Score"

* Resumen de m√©tricas  

[i031]: https://i.imgur.com/XUqvrdJ.jpg "Resumen de m√©tricas"

* Tasa de falsos negativos  

[i032]: https://i.imgur.com/5otAfdi.png "Tasa de falsos negativos"

* M√©tricas de la matriz de confusi√≥n  

[i033]: https://i.imgur.com/P6fZOdY.png "M√©tricas de la matriz de confusi√≥n"




